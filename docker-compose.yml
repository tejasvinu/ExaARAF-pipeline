services:
  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:v2.37.0
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "9099:9090"  # Changed from 9091 to 9099 (uncommon port)
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'  # Enable HTTP endpoints for runtime management
      - '--storage.tsdb.retention.time=15d'  # Set retention period
    restart: unless-stopped
    networks:
      - pipeline-network
    environment:
      - TZ=Asia/Kolkata
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Zookeeper Ensemble (3 nodes for reliability)
  zookeeper-1:
    image: confluentinc/cp-zookeeper:7.3.0
    ports:
      # Expose client port only on one node for easier debugging if needed
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 1
      # Use individual server variables instead of ZOOKEEPER_SERVERS
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
      TZ: "Asia/Kolkata"
    restart: unless-stopped
    networks:
      - pipeline-network
    volumes:
      - zookeeper_data_1:/var/lib/zookeeper/data
      - zookeeper_log_1:/var/lib/zookeeper/log
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

  zookeeper-2:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 2
      # Use individual server variables instead of ZOOKEEPER_SERVERS
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
      TZ: "Asia/Kolkata"
    restart: unless-stopped
    networks:
      - pipeline-network
    volumes:
      - zookeeper_data_2:/var/lib/zookeeper/data
      - zookeeper_log_2:/var/lib/zookeeper/log
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

  zookeeper-3:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 3
      # Use individual server variables instead of ZOOKEEPER_SERVERS
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
      TZ: "Asia/Kolkata"
    restart: unless-stopped
    networks:
      - pipeline-network
    volumes:
      - zookeeper_data_3:/var/lib/zookeeper/data
      - zookeeper_log_3:/var/lib/zookeeper/log
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

  # Kafka Cluster (3 brokers for reliability/scalability)
  kafka-1:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      zookeeper-1: { condition: service_healthy }
      zookeeper-2: { condition: service_healthy }
      zookeeper-3: { condition: service_healthy }
    ports:
      # Expose external port for clients (adjust host port if needed)
      - "9092:9092"
    volumes:
      - kafka_logs_1:/var/log/kafka # Unique log volume per broker recommended
      - kafka_data_1:/var/lib/kafka/data # Added persistent data volume
    command: >
      bash -c '
        mkdir -p /etc/kafka &&
        cp /etc/confluent/docker/log4j.properties.template /etc/kafka/log4j.properties &&
        chmod 666 /etc/kafka/log4j.properties &&
        /etc/confluent/docker/run'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      # Listeners: INTERNAL for inter-broker, EXTERNAL for clients outside docker network
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: INTERNAL://:19092,EXTERNAL://:9092
      # Advertised Listeners: Use service name for internal, host IP/DNS/localhost for external
      # !! IMPORTANT: Replace 'localhost' with your Docker host IP/DNS if clients are external to the host !!
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-1:19092,EXTERNAL://localhost:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      # Reliability settings
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      # Logging - use correct path for confluentinc image
      KAFKA_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/kafka/log4j.properties"
      KAFKA_OPTS: "-Dkafka.logs.dir=/var/log/kafka"
      # Performance tuning (adjust based on monitoring)
      KAFKA_NUM_PARTITIONS: 4 # Default partitions for auto-created topics
      KAFKA_MESSAGE_MAX_BYTES: 5000000
      KAFKA_REPLICA_FETCH_MAX_BYTES: 5000000
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_NUM_IO_THREADS: 6 # Adjusted for clustered setup
      KAFKA_NUM_NETWORK_THREADS: 6 # Adjusted for clustered setup
      KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR: 2
      KAFKA_LOG_FLUSH_INTERVAL_MESSAGES: 10000
      KAFKA_LOG_FLUSH_INTERVAL_MS: 1000
      TZ: "Asia/Kolkata"
    restart: unless-stopped
    networks:
      - pipeline-network
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    healthcheck:
      # Use internal listener port 19092 for healthcheck within container
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:19092", "--list"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s

  kafka-2:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      zookeeper-1: { condition: service_healthy }
      zookeeper-2: { condition: service_healthy }
      zookeeper-3: { condition: service_healthy }
    ports:
      # Map different host port for external access if needed
      - "9097:9092"  # Changed from 9093 to 9097 (uncommon port)
    volumes:
      - kafka_logs_2:/var/log/kafka
      - kafka_data_2:/var/lib/kafka/data
    command: >
      bash -c '
        mkdir -p /etc/kafka &&
        cp /etc/confluent/docker/log4j.properties.template /etc/kafka/log4j.properties &&
        chmod 666 /etc/kafka/log4j.properties &&
        /etc/confluent/docker/run'
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: INTERNAL://:19092,EXTERNAL://:9092
      # !! IMPORTANT: Replace 'localhost' with your Docker host IP/DNS if clients are external to the host !!
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-2:19092,EXTERNAL://localhost:9097  # Updated to match new port
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/kafka/log4j.properties"
      KAFKA_OPTS: "-Dkafka.logs.dir=/var/log/kafka"
      KAFKA_NUM_PARTITIONS: 4
      KAFKA_MESSAGE_MAX_BYTES: 5000000
      KAFKA_REPLICA_FETCH_MAX_BYTES: 5000000
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_NUM_IO_THREADS: 6
      KAFKA_NUM_NETWORK_THREADS: 6
      KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR: 2
      KAFKA_LOG_FLUSH_INTERVAL_MESSAGES: 10000
      KAFKA_LOG_FLUSH_INTERVAL_MS: 1000
      TZ: "Asia/Kolkata"
    restart: unless-stopped
    networks:
      - pipeline-network
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:19092", "--list"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s

  kafka-3:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      zookeeper-1: { condition: service_healthy }
      zookeeper-2: { condition: service_healthy }
      zookeeper-3: { condition: service_healthy }
    ports:
      # Map different host port for external access if needed
      - "9095:9092"  # Changed from 9094 to 9095
    volumes:
      - kafka_logs_3:/var/log/kafka
      - kafka_data_3:/var/lib/kafka/data
    command: >
      bash -c '
        mkdir -p /etc/kafka &&
        cp /etc/confluent/docker/log4j.properties.template /etc/kafka/log4j.properties &&
        chmod 666 /etc/kafka/log4j.properties &&
        /etc/confluent/docker/run'
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: INTERNAL://:19092,EXTERNAL://:9092
      # !! IMPORTANT: Replace 'localhost' with your Docker host IP/DNS if clients are external to the host !!
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-3:19092,EXTERNAL://localhost:9095 # Changed port to match new external port
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/kafka/log4j.properties"
      KAFKA_OPTS: "-Dkafka.logs.dir=/var/log/kafka"
      KAFKA_NUM_PARTITIONS: 4
      KAFKA_MESSAGE_MAX_BYTES: 5000000
      KAFKA_REPLICA_FETCH_MAX_BYTES: 5000000
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_NUM_IO_THREADS: 6
      KAFKA_NUM_NETWORK_THREADS: 6
      KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR: 2
      KAFKA_LOG_FLUSH_INTERVAL_MESSAGES: 10000
      KAFKA_LOG_FLUSH_INTERVAL_MS: 1000
      TZ: "Asia/Kolkata"
    restart: unless-stopped
    networks:
      - pipeline-network
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:19092", "--list"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s

  kafka-init:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      # Wait for all brokers to be healthy
      kafka-1: { condition: service_healthy }
      kafka-2: { condition: service_healthy }
      kafka-3: { condition: service_healthy }
    # Use internal broker list
    command: >
      bash -c "
        echo 'Waiting for Kafka cluster to be fully ready...' &&
        # Check for 3 brokers available, increase timeout
        cub kafka-ready -b kafka-1:19092,kafka-2:19092,kafka-3:19092 3 120 &&
        echo 'Creating Kafka topics...' &&
        kafka-topics --create --if-not-exists --bootstrap-server kafka-1:19092,kafka-2:19092,kafka-3:19092 --partitions 1 --replication-factor 3 --topic ipmi_metrics --config min.insync.replicas=2 &&
        kafka-topics --create --if-not-exists --bootstrap-server kafka-1:19092,kafka-2:19092,kafka-3:19092 --partitions 4 --replication-factor 3 --topic node_metrics --config retention.ms=86400000 --config min.insync.replicas=2 &&
        kafka-topics --create --if-not-exists --bootstrap-server kafka-1:19092,kafka-2:19092,kafka-3:19092 --partitions 1 --replication-factor 3 --topic dcgm_metrics --config min.insync.replicas=2 &&
        kafka-topics --create --if-not-exists --bootstrap-server kafka-1:19092,kafka-2:19092,kafka-3:19092 --partitions 1 --replication-factor 3 --topic slurm_metrics --config min.insync.replicas=2 &&
        echo 'Kafka topic creation complete.'
      "
    networks:
      - pipeline-network
    # No restart needed, it's an init container
    # Add resource limits for init container
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  # Prometheus-Kafka connector
  prometheus-kafka-adapter:
    image: telefonica/prometheus-kafka-adapter:1.8.0
    ports:
      - "8089:8080" # Changed from 8080 to avoid conflicts
    depends_on:
      # Wait for Kafka brokers to be healthy
      kafka-1: { condition: service_healthy }
      kafka-2: { condition: service_healthy }
      kafka-3: { condition: service_healthy }
    environment:
      # Use internal broker list
      KAFKA_BROKER_LIST: kafka-1:19092,kafka-2:19092,kafka-3:19092
      KAFKA_TOPIC_TEMPLATE: '{{.labels.job}}_metrics'
      # Optional: Add Kafka client tuning env vars if needed (compression, batching, acks)
      # KAFKA_COMPRESSION_TYPE: lz4
      # KAFKA_BATCH_NUM_MESSAGES: 1000
      # KAFKA_REQUIRED_ACKS: 1 # Or 'all' for higher durability guarantee
    restart: unless-stopped
    networks:
      - pipeline-network
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      # Assuming /health is the correct endpoint for this image
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s

  # InfluxDB for time series metrics storage
  influxdb:
    image: influxdb:2.7
    ports:
      - "8086:8086" # Main API and UI port
    volumes:
      - influxdb_data:/var/lib/influxdb2
      - influxdb_config:/etc/influxdb2
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD=adminpassword123
      - DOCKER_INFLUXDB_INIT_ORG=metrics_org
      - DOCKER_INFLUXDB_INIT_BUCKET=metrics_bucket
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=my-super-secret-auth-token
      - TZ=Asia/Kolkata
    restart: unless-stopped
    networks:
      - pipeline-network
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8086/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Telegraf for metrics processing and forwarding to InfluxDB
  telegraf:
    image: telegraf:1.28
    volumes:
      - ./telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:ro
    depends_on:
      - influxdb
      - kafka-1
      - kafka-2
      - kafka-3
    environment:
      - INFLUXDB_TOKEN=my-super-secret-auth-token
      - INFLUXDB_ORG=metrics_org
      - INFLUXDB_BUCKET=metrics_bucket
      - INFLUXDB_URL=http://influxdb:8086
      - TZ=Asia/Kolkata
    restart: unless-stopped
    networks:
      - pipeline-network
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD", "telegraf", "--test"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

networks:
  pipeline-network:
    driver: bridge

volumes:
  prometheus_data:
  zookeeper_data_1:
  zookeeper_log_1:
  zookeeper_data_2:
  zookeeper_log_2:
  zookeeper_data_3:
  zookeeper_log_3:
  kafka_logs_1:
  kafka_data_1:
  kafka_logs_2:
  kafka_data_2:
  kafka_logs_3:
  kafka_data_3:
  influxdb_data:
  influxdb_config:

# Security Note: This configuration uses PLAINTEXT communication between services (Kafka, ZK).
# For production environments, strongly consider enabling TLS/SSL encryption and authentication.

# Logging Note: Container logs will use the default Docker logging driver (usually json-file).
# Configure Docker daemon's logging options (e.g., max-size, max-file) or use a centralized logging driver for production.