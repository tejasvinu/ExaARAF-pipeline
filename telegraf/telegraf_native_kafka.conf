[global_tags]
  pipeline = "exaaraf"

[agent]
  interval = "10s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "10s"
  flush_jitter = "0s"
  precision = ""
  hostname = ""
  omit_hostname = false

[[inputs.kafka_consumer]]
  ## Kafka brokers.
  brokers = ["kafka-1:19092", "kafka-2:19092", "kafka-3:19092"]
  ## Topics to consume.
  topics = ["ipmi_metrics", "node_metrics", "dcgm_metrics", "slurm_metrics"]
  ## Consumer group name
  consumer_group = "telegraf_kafka_native_group" # Changed group name
  ## Offset (use "newest" initially to avoid backlog issues during testing)
  offset = "newest"
  # offset = "earliest" # Switch back if you need history AND it proves stable

  ## Data format parsing
  data_format = "json"
  # Define the expected JSON structure
  json_timestamp_path = "timestamp"
  # Assuming ISO8601 format like "2023-10-27T10:00:00.123456Z" or with offset
  json_timestamp_format = "2006-01-02T15:04:05.999999Z07:00"
  # Keep 'labels' as a string field for the processor
  json_string_fields = ["labels", "metric_type"]
  # Use 'name' field from JSON as the measurement name
  json_name_key = "name"
  # Add 'metric_type' as a top-level tag if desired
  tag_keys = ["metric_type"]

# Processor to parse the nested 'labels' field using Starlark
[[processors.starlark]]
  # Name for logging/debugging
  namepass = ["kafka_consumer"] # Only process metrics from the kafka input
  source = '''
load("json.star", "json")
load("logging.star", "log")

def apply(metric):
    # Check if the 'labels' field exists
    if "labels" not in metric.fields:
        return metric

    try:
        # Get the labels string
        labels_str = metric.fields["labels"]

        # Remove it from fields as we convert it to tags
        metric.fields.pop("labels")

        # Handle potential outer quotes if the string field itself was quoted
        if labels_str.startswith('"') and labels_str.endswith('"'):
            labels_str = labels_str[1:-1]

        # Replace escaped quotes
        cleaned_str = labels_str.replace('\\\\"', '"').replace('\\"', '"')

        # Parse the inner JSON
        parsed_labels = json.decode(cleaned_str)

        # Add parsed labels as tags, skipping __name__
        for k, v in parsed_labels.items():
            if k != "__name__" and v != None:
                # Ensure tag values are strings
                metric.tags[k] = str(v)

    except Exception as e:
        log.error("Failed to parse labels JSON: " + str(e))
        metric.tags["label_parse_error"] = "true"
        # Optionally keep the original string for debugging
        # metric.fields["original_labels"] = labels_str

    # Handle NaN value if necessary (JSON parser might make it None or fail)
    # Starlark doesn't have a direct NaN concept like Python
    if "value" in metric.fields:
        # Check for Python float NaN (might not occur via JSON)
        # A string "NaN" might be more likely if it came from the source that way
         if metric.fields["value"] == "NaN":
             metric.tags["value_isNaN"] = "true" # Add tag indicating NaN
             metric.fields.pop("value") # Remove the non-numeric field value

    return metric
'''

[[outputs.influxdb_v2]]
  ## The URLs of the InfluxDB cluster nodes.
  urls = ["http://influxdb:8086"]
  token = "$INFLUXDB_TOKEN"
  organization = "$INFLUXDB_ORG"
  bucket = "$INFLUXDB_BUCKET"
  timeout = "5s"
  content_encoding = "gzip"
  # metric_buffer_limit = 10000 # Already set in agent